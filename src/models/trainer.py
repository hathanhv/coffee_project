"""
Model Trainer Module for Coffee Project

Module n√†y hu·∫•n luy·ªán c√°c m√¥ h√¨nh clustering (KMeans, GMM, DBSCAN, HDBSCAN) cho b√†i to√°n
ph√¢n c·ª•m kh√°ch h√†ng Highlands Coffee.

T√≠nh nƒÉng ch√≠nh:
- Load d·ªØ li·ªáu ƒë√£ encode
- Train model v·ªõi c·∫•u h√¨nh c·ªë ƒë·ªãnh
- ƒê√°nh gi√° b·∫±ng 3 metrics: Silhouette, Calinski-Harabasz, Davies-Bouldin
- L∆∞u/load model v√† labels
"""

import logging
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Union
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
try:
    from hdbscan import HDBSCAN
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False
import joblib

from src.models.evaluator import ClusteringEvaluator


@dataclass
class TrainingConfig:
    """
    C·∫•u h√¨nh cho qu√° tr√¨nh training clustering models
    
    Attributes:
        data_path: ƒê∆∞·ªùng d·∫´n file CSV ƒë√£ encode
        model_type: Lo·∫°i model ('kmeans', 'gmm', 'dbscan', 'hdbscan')
        n_clusters: S·ªë c·ª•m (d√πng cho KMeans/GMM)
        random_state: Random seed ƒë·ªÉ reproducible
        selected_features: Danh s√°ch t√™n c·ªôt c·∫ßn ch·ªçn. N·∫øu None, d√πng t·∫•t c·∫£
        results_path: ƒê∆∞·ªùng d·∫´n l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° (CSV)
        model_path: ƒê∆∞·ªùng d·∫´n l∆∞u model (PKL)
        model_params: Dict ch·ª©a hyperparams ri√™ng cho t·ª´ng model
            - KMeans: {"n_init": 20, "max_iter": 500}
            - GMM: {"covariance_type": "full"}
            - DBSCAN: {"eps": 0.7, "min_samples": 10}
            - HDBSCAN: {"min_cluster_size": 15, "min_samples": 10}
    """
    data_path: str = "data/processed/encoded_data.csv"
    model_type: str = "kmeans"  # 'kmeans', 'gmm', 'dbscan', 'hdbscan'
    n_clusters: int = 5
    random_state: int = 42
    selected_features: Optional[List[str]] = None
    results_path: str = "results/cluster_results.csv"
    model_path: str = "results/best_cluster_model.pkl"
    model_params: Dict[str, Any] = field(default_factory=dict)


class ModelTrainer:
    """
    Class ƒë·ªÉ hu·∫•n luy·ªán c√°c m√¥ h√¨nh clustering
    
    Workflow:
    1. load_data()      - Load d·ªØ li·ªáu ƒë√£ encode
    2. build_model()    - Kh·ªüi t·∫°o model theo config
    3. train_model()    - Fit model v·ªõi c·∫•u h√¨nh c·ªë ƒë·ªãnh
    4. evaluate()       - ƒê√°nh gi√° model hi·ªán t·∫°i
    5. save_model()     - L∆∞u model
    
    Attributes:
        config: TrainingConfig object
        evaluator: ClusteringEvaluator object
        logger: Logger instance
        df: DataFrame ch·ª©a d·ªØ li·ªáu
        X: Numpy array features
        model: Model ƒë√£ train
    
    Example:
        >>> config = TrainingConfig(
        ...     model_type='kmeans',
        ...     n_clusters=5,
        ...     model_params={"n_init": 20}
        ... )
        >>> evaluator = ClusteringEvaluator()
        >>> trainer = ModelTrainer(config, evaluator)
        >>> trainer.load_data()
        >>> trainer.train_model()
        >>> metrics = trainer.evaluate()
        >>> trainer.save_model()
    """
    
    def __init__(
        self,
        config: TrainingConfig,
        evaluator: ClusteringEvaluator,
        logger: Optional[logging.Logger] = None
    ):
        """
        Kh·ªüi t·∫°o ModelTrainer
        
        Args:
            config: TrainingConfig object ch·ª©a c·∫•u h√¨nh
            evaluator: ClusteringEvaluator ƒë·ªÉ ƒë√°nh gi√° models
            logger: Logger instance. N·∫øu None, t·∫°o logger m·∫∑c ƒë·ªãnh
        """
        self.config = config
        self.evaluator = evaluator
        
        # Kh·ªüi t·∫°o logger
        if logger is None:
            self.logger = logging.getLogger("ModelTrainer")
            self.logger.setLevel(logging.INFO)
            self.logger.propagate = False
            
            if not self.logger.handlers:
                console_handler = logging.StreamHandler()
                console_handler.setLevel(logging.INFO)
                formatter = logging.Formatter('%(message)s')
                console_handler.setFormatter(formatter)
                self.logger.addHandler(console_handler)
        else:
            self.logger = logger
        
        # Kh·ªüi t·∫°o attributes
        self.df: Optional[pd.DataFrame] = None
        self.X: Optional[np.ndarray] = None
        self.model: Optional[Union[KMeans, GaussianMixture, DBSCAN, 'HDBSCAN']] = None
        
        self.logger.info("‚úì ModelTrainer initialized")
        self.logger.info(f"  Model type: {self.config.model_type}")
        self.logger.info(f"  Data path: {self.config.data_path}")
    
    def load_data(self) -> None:
        """
        Load d·ªØ li·ªáu ƒë√£ encode t·ª´ CSV
        
        ƒê·ªçc file CSV, ch·ªçn features n·∫øu c·∫ßn, chuy·ªÉn th√†nh numpy array
        """
        self.logger.info(f"üìÇ Loading data from {self.config.data_path}...")
        
        if not os.path.exists(self.config.data_path):
            raise FileNotFoundError(f"File kh√¥ng t·ªìn t·∫°i: {self.config.data_path}")
        
        # Load CSV
        self.df = pd.read_csv(self.config.data_path)
        
        # Ch·ªçn l·ªçc features n·∫øu c·∫ßn
        if self.config.selected_features is not None:
            missing_cols = set(self.config.selected_features) - set(self.df.columns)
            if missing_cols:
                raise ValueError(f"C√°c c·ªôt kh√¥ng t·ªìn t·∫°i: {missing_cols}")
            
            self.df = self.df[self.config.selected_features]
            self.logger.info(f"  ‚úì Selected {len(self.config.selected_features)} features")
        
        # Chuy·ªÉn th√†nh numpy array
        self.X = self.df.values
        
        self.logger.info(f"  ‚úì Data loaded: {self.X.shape[0]} samples, {self.X.shape[1]} features")
    
    def build_model(self, n_clusters: Optional[int] = None) -> Union[KMeans, GaussianMixture, DBSCAN, 'HDBSCAN']:
        """
        Kh·ªüi t·∫°o m√¥ h√¨nh clustering v·ªõi c·∫•u h√¨nh t·ª´ config
        
        Args:
            n_clusters: S·ªë c·ª•m (d√πng cho KMeans/GMM). N·∫øu None, d√πng config.n_clusters
        
        Returns:
            Model instance ƒë√£ kh·ªüi t·∫°o
        
        Raises:
            ValueError: N·∫øu model_type kh√¥ng h·ª£p l·ªá
            ImportError: N·∫øu HDBSCAN ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t
        """
        model_type = self.config.model_type.lower()
        params = dict(self.config.model_params)  # Copy ƒë·ªÉ kh√¥ng s·ª≠a dict g·ªëc
        
        if model_type == "kmeans":
            default_params = {
                "n_clusters": n_clusters or self.config.n_clusters,
                "n_init": "auto",
                "random_state": self.config.random_state,
            }
            default_params.update(params)
            return KMeans(**default_params)
        
        elif model_type == "gmm":
            default_params = {
                "n_components": n_clusters or self.config.n_clusters,
                "random_state": self.config.random_state,
            }
            default_params.update(params)
            return GaussianMixture(**default_params)
        
        elif model_type == "dbscan":
            default_params = {
                "eps": 0.5,
                "min_samples": 5,
                "n_jobs": -1,
            }
            default_params.update(params)
            return DBSCAN(**default_params)
        
        elif model_type == "hdbscan":
            if not HDBSCAN_AVAILABLE:
                raise ImportError(
                    "HDBSCAN ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i b·∫±ng: pip install hdbscan"
                )
            default_params = {
                "min_cluster_size": 5,
                "min_samples": None,
                "core_dist_n_jobs": -1,
            }
            default_params.update(params)
            return HDBSCAN(**default_params)
        
        else:
            raise ValueError(
                f"model_type '{self.config.model_type}' kh√¥ng h·ª£p l·ªá. "
                f"Ch·ªçn 'kmeans', 'gmm', 'dbscan', ho·∫∑c 'hdbscan'."
            )
    
    def train_model(self) -> None:
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi c·∫•u h√¨nh ƒë√£ cho
        
        Build model t·ª´ config v√† fit tr√™n to√†n b·ªô d·ªØ li·ªáu X
        """
        if self.X is None:
            raise ValueError("Ch∆∞a load d·ªØ li·ªáu! G·ªçi load_data() tr∆∞·ªõc.")
        
        model_type = self.config.model_type.lower()
        
        if model_type in ['dbscan', 'hdbscan']:
            self.logger.info(f"üîß Training {model_type.upper()}...")
        else:
            self.logger.info(f"üîß Training {model_type.upper()} with {self.config.n_clusters} clusters...")
        
        self.model = self.build_model()
        self.model.fit(self.X)
        
        self.logger.info("  ‚úì Model trained successfully")
    
    def evaluate(self) -> Dict[str, float]:
        """
        ƒê√°nh gi√° m√¥ h√¨nh hi·ªán t·∫°i b·∫±ng ClusteringEvaluator
        
        Returns:
            Dict ch·ª©a c√°c metrics:
            {
                'model': str,
                'n_clusters': int,
                'silhouette': float,
                'calinski_harabasz': float,
                'davies_bouldin': float
            }
        """
        if self.model is None:
            raise ValueError("Ch∆∞a c√≥ model! G·ªçi train_model() tr∆∞·ªõc.")
        
        if self.X is None:
            raise ValueError("Ch∆∞a load d·ªØ li·ªáu! G·ªçi load_data() tr∆∞·ªõc.")
        
        self.logger.info("üìä Evaluating model...")
        
        # L·∫•y labels
        labels = self.get_cluster_labels()
        
        # ƒê√°nh gi√°
        metrics = self.evaluator.evaluate_once(
            X=self.X,
            labels=labels,
            model_name=self.config.model_type
        )
        
        # Log metrics
        self.logger.info(f"  ‚úì Silhouette Score    : {metrics['silhouette']:>7.4f}")
        self.logger.info(f"  ‚úì Calinski-Harabasz   : {metrics['calinski_harabasz']:>7.2f}")
        self.logger.info(f"  ‚úì Davies-Bouldin Index: {metrics['davies_bouldin']:>7.4f}")
        
        return metrics
    
    def save_model(self, path: Optional[str] = None) -> None:
        """
        L∆∞u model v√†o file PKL
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n file PKL. N·∫øu None, d√πng config.model_path
        """
        if self.model is None:
            raise ValueError("Ch∆∞a c√≥ model ƒë·ªÉ l∆∞u! G·ªçi train_model() tr∆∞·ªõc.")
        
        save_path = path if path is not None else self.config.model_path
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        joblib.dump(self.model, save_path)
        self.logger.info(f"üíæ Model saved: {save_path}")
    
    @staticmethod
    def load_model(path: str) -> Union[KMeans, GaussianMixture, DBSCAN, 'HDBSCAN']:
        """
        Load model t·ª´ file PKL
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n file PKL
        
        Returns:
            Model instance
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"File kh√¥ng t·ªìn t·∫°i: {path}")
        
        return joblib.load(path)
    
    def get_cluster_labels(self) -> np.ndarray:
        """
        L·∫•y cluster labels t·ª´ model
        
        Returns:
            Numpy array ch·ª©a cluster labels
            (DBSCAN/HDBSCAN c√≥ th·ªÉ c√≥ label -1 cho noise points)
        """
        if self.model is None:
            raise ValueError("Ch∆∞a c√≥ model! G·ªçi train_model() tr∆∞·ªõc.")
        
        if self.X is None:
            raise ValueError("Ch∆∞a load d·ªØ li·ªáu! G·ªçi load_data() tr∆∞·ªõc.")
        
        if self.config.model_type.lower() in ['dbscan', 'hdbscan']:
            return self.model.fit_predict(self.X)
        elif hasattr(self.model, 'labels_'):
            return self.model.labels_
        else:
            return self.model.predict(self.X)
    
    def save_labels(self, output_path: str) -> None:
        """
        L∆∞u cluster labels ra file CSV
        
        Args:
            output_path: ƒê∆∞·ªùng d·∫´n file CSV ƒë·ªÉ l∆∞u labels
        """
        labels = self.get_cluster_labels()
        
        df_labels = self.df.copy()
        df_labels['cluster'] = labels
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        df_labels.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        self.logger.info(f"üíæ Labels saved: {output_path}")


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    """
    Demo script ƒë·ªÉ test ModelTrainer
    """
    from src.models.evaluator import ClusteringEvaluator
    
    # Example 1: Train KMeans v·ªõi custom params
    config_kmeans = TrainingConfig(
        data_path="data/processed/encoded_data.csv",
        model_type="kmeans",
        n_clusters=5,
        model_params={"n_init": 20, "max_iter": 500}
    )
    
    evaluator = ClusteringEvaluator()
    trainer = ModelTrainer(config=config_kmeans, evaluator=evaluator)
    
    trainer.load_data()
    trainer.train_model()
    metrics = trainer.evaluate()
    trainer.save_model()
    trainer.save_labels("results/kmeans_labels.csv")
    
    print("\n" + "="*70)
    print("KMeans Metrics:")
    for key, value in metrics.items():
        print(f"  {key}: {value}")
    
    # Example 2: Train DBSCAN
    config_dbscan = TrainingConfig(
        data_path="data/processed/encoded_data.csv",
        model_type="dbscan",
        model_params={"eps": 2.0, "min_samples": 10}
    )
    
    trainer_dbscan = ModelTrainer(config=config_dbscan, evaluator=evaluator)
    trainer_dbscan.load_data()
    trainer_dbscan.train_model()
    metrics_dbscan = trainer_dbscan.evaluate()
    
    print("\n" + "="*70)
    print("DBSCAN Metrics:")
    for key, value in metrics_dbscan.items():
        print(f"  {key}: {value}")
    
    # Example 3: Train GMM
    config_gmm = TrainingConfig(
        data_path="data/processed/encoded_data.csv",
        model_type="gmm",
        n_clusters=4,
        model_params={"covariance_type": "full"}
    )
    
    trainer_gmm = ModelTrainer(config=config_gmm, evaluator=evaluator)
    trainer_gmm.load_data()
    trainer_gmm.train_model()
    metrics_gmm = trainer_gmm.evaluate()
    
    print("\n" + "="*70)
    print("GMM Metrics:")
    for key, value in metrics_gmm.items():
        print(f"  {key}: {value}")

